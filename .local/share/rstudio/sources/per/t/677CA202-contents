---
title: "Simple Linear Regression"
author: "Frederick De Baene"
date: "2024-11-19"
output:
  html_document:
    toc: true
    df_print: paged
  pdf_document:
    number_sections: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE}
set.seed(123)
```

# Introduction

The simple linear regression model is represented as follows:

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

The following restrictions are enforced:

- $E[\epsilon_i] = 0$
- $Var(\epsilon_i) = \sigma^2$
- $\epsilon_i$ is normally distributed
- $Cov(\epsilon_i, \epsilon_j) = 0$ for $i \neq j$.

The responses are drawn from conditional probability distributions. These 
conditional probability distributions condition on the level of the predictor 
variable. The mean of the conditional probability distributions of the response 
depends on the level of the predictor. The regression function relates the 
expected value for the response to the predictor value:

$$
E[Y_i] = E[\beta_0 + \beta_1 x_i + \epsilon_i] = E[\beta_0] + E[\beta_1 x_i] + E[\epsilon_i] = \beta_0 + \beta_1 x_i.
$$

Remember that $E[\epsilon_i] = 0$.

# Simulations

We assume the following simple linear regression model for the population:

$$
Y = 10 + 2X + \epsilon_i.
$$

In this example, we consider a sample comprising 10 observations. For each observation, 
the level for $X$ is fixed. We repeatedly sample an outcome for each observation from 
their respective conditional probability distributions for the outcome.

```{r}
# Initialize the population parameters and the simulation size
beta_0 <- 10
beta_1 <- 2
err_var <- 4
n <- 10
sim_size <- 10000

# Initialize a vector with varying levels for the predictor
x <- seq(from = 0, to = 90, by = 10)

# Initialize a matrix to hold the outcomes for the simulations
y <- matrix(
  data = rep(NA, times = length(x) * sim_size),
  nrow = length(x), ncol = sim_size
)
```

```{r}
# During each simulation, for each level of the predictor, sample from the 
# conditional probability distribution of the response
for (i in 1:sim_size) y[, i] <- beta_0 + beta_1 * x + rnorm(n = n, sd = sqrt(err_var))
```

## Assumptions and Restrictions

### Independent Errors

The assumption of independent and identically distributed errors is stated as follows:

$$
\epsilon_{i} \sim \mathcal{N}(0, \sigma^2)
$$

and

$$
Cov(\epsilon_i, \epsilon_j) = 0 \space \space with \space \space i \neq j.
$$

Note that $\sigma^2$ is the variance of the conditional probability distributions of $Y$ 
across all levels of $X$. Later on, we will see that one of the assumptions of linear regression 
states that the variance for all conditional probability distributions of $Y$ is equal. Therefore, 
we do not have to write $\sigma_{i}^{2}$ but we can just write $\sigma^2$.

**Simulation setup.** In the simulation setup, we make use of a sample comprising 10 
observations. For each observation, we keep the level of $X$ fixed. Suppose we zoom in 
on observations $i = 3$ and $i = 5$. Their predictor values are:

```{r}
x[3]
x[5]
```

Given that the population parameters are known, we can determine the mean of the conditional 
probability distributions for $i = 3$ and $i = 5$ given $X$:

$$
\begin{aligned}
E(Y_{i} | x_{i}) &= \beta_0 + \beta_1 \times x_{i} \\
&= 10 + 2 \times x_{i} \\
\end{aligned}
$$

This gives us the following:

$$
\begin{aligned}
E(Y_{3} | x_{3}) = E(Y_{3} | 20) 
&= 10 + 2 \times 20 \\
&= 50
\end{aligned}
$$

and

$$
\begin{aligned}
E(Y_{5} | x_{5}) = E(Y_{5} | 40) 
&= 10 + 2 \times 40 \\
&= 90
\end{aligned}
$$

Given the means of the conditional probability distributions of $Y$ for $i = 3$ and $i = 5$, 
we can draw the curves representing the conditional probability distributions for these two 
observations. From the population model we know that $\sigma^2 = 4$.

```{r echo = FALSE}
x_3_mean <- 50
x_3 <- seq(from = 40, to = 60, length = 1000)
y_3 <- dnorm(x_3, mean = x_3_mean, sd = sqrt(err_var))
plot(x_3, y_3, xlab = "Y3", ylab = "")
```

Looking at the actual outcomes, we observe the following distribution for $Y_3$:

```{r echo = FALSE}
hist(y[3, ], breaks = 50, main = "Distribution for Y3")
```


```{r echo = FALSE}
x_5_mean <- 90
x_5 <- seq(from = 80, to = 100, length = 1000)
y_5 <- dnorm(x_5, mean = x_5_mean, sd = sqrt(err_var))
plot(x_5, y_5, xlab = "Y5", ylab = "")
```

Looking at the actual outcomes, we observe the following distribution for $Y_5$:

```{r echo = FALSE}
hist(y[5, ], breaks = 50, main = "Distribution for Y5")
```

The error terms are independent. This implies that the error terms for $i = 3$ are not 
correlated with the error terms for $i = 5$. The following scatter plot indicates that 
the error terms are independent:

```{r echo = FALSE}
i_3_res <- y[3, ] - x_3_mean
i_5_res <- y[5, ] - x_5_mean
plot(i_3_res, i_5_res, xlab = "Residuals (i = 3)", ylab = "Residuals (i = 5)")
abline(h = 0, col = "red")
abline(v = 0, col = "red")
```

The covariance $Cov(\epsilon_{3}, \epsilon_{5}) = `r round(cov(i_3_res, i_5_res), 3)`$ and 
the scatter plot indicate no correlation between the error terms for $i = 3$ and $i = 5$. Also 
note the horizontal and vertical red lines, which seem to indicate that the mean for $\epsilon_{3}$ 
and $\epsilon_{5}$ is 0, in accordance with the assumption $\epsilon_{i} \sim \mathcal{N}(0, \sigma^2)$.

A variance-covariance matrix is a square matrix. The diagonal elements equal 
the variance of the error terms $\sigma^2$, while the off-diagonal elements 
equal the covariance $Cov(\epsilon_i, \epsilon_j)$ with $i \neq j$.

```{r}
vars <- rep(NA, times = n)
for (i in 1:n) vars[i] <- var(y[i, ])
```

## Variance

The variance $\sigma^2$ of the conditional probability distributions of $Y$ is the same for each 
level of $X$. A point estimator for $\sigma^2$ is the mean squared error (MSE). We must take 
into account that the deviations for each observation are the deviations between the observations 
and the mean of their respective conditional probability distribution.

For $i = 3$, the mean of the conditional probability distribution is 50. Therefore, to calculate 
the deviance for $i = 3$, we do:

$$
Y_{3} - E(Y_{3} | 20) = Y_{3} - 50
$$

For $i = 5$, we have:

$$
Y_{5} - E(Y_{5} | 40) = Y_{5} - 90
$$

To calculate the sum of squared errors taking into account the means of the conditional 
probability distributions, we utilize the residuals (or errors):

$$
E_{i} = Y_{i} - E(Y_{i} | X_{i}) = Y_{i} - \hat{Y}_{i}
$$

The sum of squared errors is calculated as follows:

$$
SSE = \sum_{i = 1}^{n} (Y_{i} - \hat{Y}_{i})^2
$$

To obtain the variance, we divide the sum of squares by its associated number of degrees 
of freedom. For a simple linear regression model, this is $n - 2$, because we need two 
parameters $\beta_0$ and $\beta_1$ to calculate (or estimate) the means of the conditional 
probability distributions of $Y$. This gives us:

$$
MSE = \frac{SSE}{n - 2} = \frac{\sum_{i = 1}^{n} (Y_{i} - \hat{Y}_{i})^2}{n - 2}
$$

The mean squared error is a point estimator for $\sigma^2$:

$$
s^2 = MSE
$$

We know that $\sigma^2 = `r err_var`$. Now let's verify that the mean squared error 
is an appropriate point estimator for $\sigma^2$:

```{r}
# First, calculate the mean of the conditional probability distributions of Y
estimated_means <- beta_0 + beta_1 * x

# Second, calculate the residuals
errors <- sweep(y, MARGIN = 1, STATS = estimated_means, FUN = "-")

# Third, sum the squared errors and divide by the appropriate degress of freedom
mse <- sum(errors^2) / ((n * sim_size) - 2)
```

We see that our estimate $s^2 = `r round(mse, 2)`$ approximates $\sigma^2 = `r err_var`$.

# Inference

Throughout this chapter, we assume the normal error regression model holds:

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where $\beta_0$ and $\beta_1$ are unknown population parameters, $x_i$ are known constants, 
and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$.

## $\beta_1$

Drawing inferences on $\beta_1$ comprises either interval estimation or hypothesis testing of the form:

$$
H_o : \beta_1 = 0
$$

versus

$$
H_a: \beta_1 \neq 0
$$

If $\beta_1 = 0$, there is no linear association between $X$ and $Y$. The means of the 
conditional probability distributions of $Y$ is the same for every level of $X$.

$$
E[Y] = \beta_0 + 0 \times X = \beta_0
$$

### Sampling Distribution

The estimator $B1$ can be used to produce a point estimate for $\beta_1$. We will use the 
samples from our simulation to produce the point estimates for $\beta_1$ (and also for $\beta_0$).
\
```{r}
slr_b1_estimates <- rep(NA, times = sim_size)
slr_b0_estimates <- rep(NA, times = sim_size)
slr_sse <- rep(NA, times = sim_size)
slr_mse <- rep(NA, times = sim_size)
slr_b1_var_estimates <- rep(NA, times = sim_size)
slr_b1_se_estimates <- rep(NA, times = sim_size)

for (i in 1:sim_size) {
  
  fit <- glm(y[, i] ~ x, family = gaussian)
  
  slr_b0_estimates[i] <- fit$coefficients[1]
  slr_b1_estimates[i] <- fit$coefficients[2]
  slr_sse[i] <- crossprod(fit$residuals)
  slr_mse[i] <- slr_sse[i] / (n - 2)
  slr_b1_var_estimates[i] <- slr_mse[i] / sum((x - mean(x))^2)
  slr_b1_se_estimates[i] <- sqrt(slr_b1_var_estimates[i])
  
}
```

The point estimates of $\beta_1$ can be used to create a sampling distribution of $B1$. The 
sampling distribution of $B1$ follows a normal distribution. Its mean equals the true value 
of the unknown population parameter $\beta_1$. The variance of the sampling distribution 
is defined as follows:

$$
Var(B1) = \frac{\sigma^2}{\sum_{i = 1}^{n} (x_i - \overline{x})^2}
$$

We know how the variance of the estimator $B1$ is calculated. Given that we know the 
true variance of the errors $\sigma^2 = `r err_var`$, let's calculate the true variance 
of the sampling distribution:

$$
\begin{aligned}
Var(B1) &= \frac{\sigma^2}{\sum_{i = 1}^{n} (x_i - \overline{x})^2} \\
&= \frac{4}{8250} \\
&= 0.0004848485
\end{aligned}
$$

```{r echo = FALSE}
b1_var <- err_var / sum((x - mean(x))^2)
b1_se <- sqrt(b1_var)
```

We obtain a variance for $B1$ of $Var(B1) = `r round(b1_var, 3)`$, which gives us a 
standard error for $B1$ of $SE(B1) = `r round(b1_se, 3)`$. Let us verify. Approximately 2.5% 
of the point estimates for $\beta_1$ must be less than $2 - 1.96 \times `r round(B1_se, 3)`$ and 
2.5% of the point estimates must be greater than $2 + 1.96 \times `r round(b1_se, 3)`$

```{r}
mean(slr_b1_estimates < (beta_1 - 1.96*b1_se))
```

```{r}
mean(slr_b1_estimates > (beta_1 + 1.96*b1_se))
```

Let us have a look again at the sampling distribution of $B1$:

```{r echo = FALSE}
hist(slr_b1_estimates, breaks = 50, main = "Sampling Distribution of B1")
abline(v = beta_1, col = "red")
abline(v = beta_1 - 1.96*b1_se, col = "blue")
abline(v = beta_1 + 1.96*b1_se, col = "blue")
```

We will verify if the sampling distribution of $B1$ follows a normal distribution:

```{r}
qqnorm(slr_b1_estimates)
```

The normality of the sampling distribution of $B1$ follows from the fact that a linear combination 
of the outcomes $Y_i$, which are independent and normally distributed, is also normally distributed. 
Furthermore, because $B1$ is an unbiased estimator of $\beta_1$, this implies that $E[B1] = \beta_1$, and, 
thus, that the mean of the sampling distribution of $B1$ equals $\beta_1$.

Standardizing $V1$ gives us a standardized statistic. Because $B1$ follows a normal 
distribution, the standardization of $B1$ results in a standard normal distribution.

$$
\frac{B1 - \beta_1}{SE(B1)} \sim \mathcal{N}(0, 1)
$$


```{r}
# We standardize B1 to obtain the standardized statistics
slr_b1_estimates_standardized <- (slr_b1_estimates - beta_1) / b1_se
```

```{r echo = FALSE}
hist(slr_b1_estimates_standardized, breaks = 50, main = "Sampling Distribution of Standardized B1")
```

Let's also verify if the distribution of the standardized statistic follows a standard 
normal distribution:

```{r}
qqnorm(slr_b1_estimates_standardized)
```

However, in practice, to standardize $B1$, we do not know the true $\sigma^2$. Instead, we 
use a point estimate that is produced using the MSE. For every fitted model, we obtained the 
MSE, and we will use this to standardize $B1$. If a variable is standardized not using the 
true standard deviation but an estimated deviation, then it is said the variable is 
studentized instead of standardized.

```{r}
# We studentize B1 to obtain the studentized statistics
slr_b1_estimates_studentized <- (slr_b1_estimates - beta_1) / slr_b1_se_estimates
```

```{r echo = FALSE}
hist(slr_b1_estimates_studentized, breaks = 50, main = "Sampling Distribution of Studentized B1")
```

Let's check if it follows a normal distribution:

```{r}
qqnorm(slr_b1_estimates_studentized)
```

This does not seem to be the case. Let's check if it follows a _t_ distribution with 
$n - 2$ degrees of freedom.

```{r}
theoretical_quantiles <- qt(ppoints(length(slr_b1_estimates_studentized)), df = 8)
```

```{r}
qqplot(theoretical_quantiles, slr_b1_estimates_studentized)
```

We now know that the studentized statistic follows a _t_-distribution with $n - 1$ degrees of 
freedom. We can use this knowledge to construct a 95% confidence interval (95% CI):

$$
B1 \pm t(1 - \alpha / 2; n - 2) \times SE(B1)
$$

We will now do this for the `r sim_size` samples we have.
\
```{r}
# Calculate the lower and upper limits for the confidence intervals
slr_b1_ci_lower_bounds <- slr_b1_estimates - slr_b1_se_estimates * qt(1 - 0.05/2, df = n - 2)
slr_b1_ci_upper_bounds <- slr_b1_estimates + slr_b1_se_estimates * qt(1 - 0.05/2, df = n - 2)
```

The interpretation of a 95% CI is as follows: When repeatedly sampling from the known population 
regression model, 95% of the confidence intervals will cover the true parameter value.
\
```{r}
mean((beta_1 > slr_b1_ci_lower_bounds) & (beta_1 < slr_b1_ci_upper_bounds))
```

If we look at the following fit, then let's construct the 95% CI.
\
```{r}
summary(fit)
```

We see that the estimated standard error for $B1$ equals 0.01504. This is calculated 
as follows:

$$
s^(B1) = \sqrt{\frac{MSE}{\sum (x_i - \overline{x})^2}}
$$

Let's calculate it using the actual data:
\
```{r}
sqrt((crossprod(fit$residuals) / (n - 2)) / sum((x - mean(x))^2))
```






